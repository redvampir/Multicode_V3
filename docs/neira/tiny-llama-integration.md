# Интеграция TinyLlama

## Навигация
- [Обзор Нейры](README.md)
- [Узлы действий](action-nodes.md)
- [Узлы анализа](analysis-nodes.md)
- [Узлы памяти](memory-nodes.md)
- [Архитектура анализа](analysis-architecture.md)
- [Поддерживающие системы](support-systems.md)
- [Личность Нейры](personality.md)
- [Шаблон узла](node-template.md)
- [Политика источников](source-policy.md)

## Оглавление
- [Подготовка исходника](#подготовка-исходника)
- [Переплавка](#переплавка)
- [Модульное хранение](#модульное-хранение)
- [Контролируемое дообучение](#контролируемое-дообучение)
- [Переход к собственным весам](#переход-к-собственным-весам)

## Подготовка исходника
Используйте открытые веса TinyLlama (Apache 2.0) в формате `safetensors`. Для гибкости храните слои модели в отдельных файлах, чтобы загрузчик мог подгружать только необходимые части и не держать в памяти весь чекпоинт.

## Переплавка
1. **Сброс токен‑эмбеддингов и LM‑head.** Эти блоки несут основную массу исходного корпуса, поэтому обнуляются перед повторным обучением.
2. **Разбиение весов на отдельные слои** (`layer_00.safetensors`, `layer_01.safetensors`, …). Такой шардированный формат позволит обновлять или заменять любые слои без перепаковки всей модели.
3. **Внедрение служебных токенов** `<chat>`, `<code>`, `<self_improve>`. Примеры их использования включаются в тренировочные данные, чтобы модель могла переключаться между режимами в экосистеме Нейры.
4. **Повторное обучение на локальном датасете**. Датасет фильтруется и дополняется правилами ответов «не знаю». Модель запоминает только допустимые знания и поведенческие инструкции, удаляя «чужой» корпус.

## Модульное хранение
Весовые файлы хранятся по слоям. Дополнительные специализированные слои (например, для отдельных языков программирования) подключаются как плагины, не затрагивая базовый набор.

## Контролируемое дообучение
Все последующие улучшения проходят через Self‑Improve Node. Он формирует патч корпуса, запускает дообучение и предлагает изменения разработчику. Логи обучения и новые веса сохраняются отдельно, чтобы при откате не затрагивать базовую модель.

## Переход к собственным весам
По мере накопления внутренних данных старые слои TinyLlama постепенно заменяются новыми, обученными внутри Нейры. В конечном итоге модель полностью переходит на собственные веса, а TinyLlama остаётся лишь историческим стартом.

